{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Environment Creation and Testing\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This is a Group assignment centered on showcasing your technical creativity skills in creating a functional or running environment for implementing AI experiments. The AI implementation environment is to be created on a single node with below mentioned libraries and tools. This will simulate an on-premise hardware environment which will be your machine. This environment should be running specifically on Linux platform (use any preferable distribution and those with Windows Operating System are expected virtualize their environment for Linux installation):\n",
    "\n",
    "1. Latest Anaconda installation.\n",
    "2. Latest version of Python\n",
    "3. Jupyter Notebook\n",
    "4. TensorFlow\n",
    "5. Keras\n",
    "6. NumPy\n",
    "7. SciPy\n",
    "8. Matplotlib\n",
    "9. Pandas\n",
    "10. Scikit-Learn\n",
    "11. Other.\n",
    "\n",
    "### After creating your environment, you should be able to provide:\n",
    "\n",
    "1. A clear demonstration and explanation of the above mentioned libraries and tools through a specific dataset of your choice. You are free to use any datasets within an African context to provide proper data manipulation operations and procedures as a way of demonstrating functionality of your created environment.\n",
    "\n",
    "2. Necessary comments on your Notebook.\n",
    "\n",
    "3. Documentation of your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Linux Platform\n",
    "\n",
    "This Group assignment will simulate an on-premise hardware environment which will be your machine. \n",
    "This environment should be running specifically on Linux platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:\tUbuntu 18.04.5 LTS\r\n"
     ]
    }
   ],
   "source": [
    "# Linux Installation\n",
    "\n",
    "# lsb_release script gives information about the Linux Standards Base (LSB) status of the distribution\n",
    "# -d gives the description of this distribution\n",
    "!lsb_release -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Required Tools and Libraries\n",
    "\n",
    "The AI implementation environment is to be created on a single node with below mentioned libraries and tools.\n",
    "\n",
    "### 2.1 Environment checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: conda: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# Latest Anaconda Installation\n",
    "\n",
    "# conda list will list all packages in the current environment\n",
    "# to check which anaconda version we have installed, we add anaconda$ that will return only the package named \"anaconda\"\n",
    "!conda list anaconda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.0\r\n"
     ]
    }
   ],
   "source": [
    "# Latest Version of Python\n",
    "\n",
    "!python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter core     : 4.7.0\n",
      "jupyter-notebook : 6.2.0\n",
      "qtconsole        : 5.0.1\n",
      "ipython          : 7.16.1\n",
      "ipykernel        : 5.4.3\n",
      "jupyter client   : 6.1.11\n",
      "jupyter lab      : not installed\n",
      "nbconvert        : 6.0.7\n",
      "ipywidgets       : 7.6.3\n",
      "nbformat         : 5.1.2\n",
      "traitlets        : 4.3.3\n"
     ]
    }
   ],
   "source": [
    "# Jupyter Notebook Installation\n",
    "\n",
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Libraries checklist\n",
    "\n",
    "After creating your environment, you should be able to provide:\n",
    "\n",
    "* A clear demonstration and explanation of the above mentioned libraries and tools through a specific dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f8ccc8ccc889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# TensorFlow Installation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensorflow: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Check the versions of libraries\n",
    "\n",
    "# TensorFlow Installation\n",
    "import tensorflow\n",
    "print(\"Tensorflow: {}\".format(tensorflow.__version__))\n",
    "\n",
    "# Keras Installation\n",
    "import keras\n",
    "print(\"Keras: {}\".format(keras.__version__))\n",
    "\n",
    "# Numpy Installation\n",
    "import numpy\n",
    "print(\"Numpy: {}\".format(numpy.__version__))\n",
    "\n",
    "# SciPy Installation\n",
    "import scipy\n",
    "print('Scipy: {}'.format(scipy.__version__))\n",
    "\n",
    "# Matplotlib Installation\n",
    "import matplotlib\n",
    "print('Matplotlib: {}'.format(matplotlib.__version__))\n",
    "\n",
    "# Pandas Installation\n",
    "import pandas\n",
    "print('Pandas: {}'.format(pandas.__version__))\n",
    "\n",
    "# Scikit-Learn Installation\n",
    "import sklearn\n",
    "print('Sklearn: {}'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load The Data\n",
    "\n",
    "You are free to use any datasets within an African context to provide proper data manipulation operations and procedures as a way of demonstrating functionality of your created environment.\n",
    "\n",
    "The dataset we will use contains information about Covid-19 death cases in Africa, per country, per day from the beginning of the pandemic.\n",
    "\n",
    "* Dataset link for download or access: https://data.humdata.org/dataset/africa-covid-19-death-cases\n",
    "\n",
    "\n",
    "Open an editing session for the project, then choose the file you want to upload. \n",
    "\n",
    "* In Jupyter Notebook, click Upload and select the file to upload. \n",
    "* Then click the blue Upload button displayed in the fileâ€™s row to add the file to the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and upload excel file \n",
    "\n",
    "# Listing files in working directory\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a file is in the project, you can use code to read it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Dataset\n",
    "\n",
    "We are using pandas to load the data. We will also use pandas next to explore the data both with descriptive statistics and data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pandas\n",
    "\n",
    "\n",
    "# Loading the covid deaths dataset from an excel file into a pandas DataFrame\n",
    "covid_df = pd.read_excel(\"covid19_africa_deceased_hera.xlsx\", index_col=0, header=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summarize the Dataset\n",
    "\n",
    "In this step we are going to take a look at the data a few different ways:\n",
    "\n",
    "1. Dimensions of the dataset.\n",
    "2. Peek at the data itself.\n",
    "3. Statistical summary of all attributes.\n",
    "4. Breakdown of the data by the class variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Dimensions of Dataset\n",
    "\n",
    "We can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape stores the number of rows and columns as a tuple (number of rows, number of columns) \n",
    "covid_df.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Peek at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing column variable names\n",
    "\n",
    "# returns the column labels of the given Dataframe\n",
    "print(covid_df.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check first 15 rows of data\n",
    "covid_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a Series with the data type of each column\n",
    "covid_df.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Statistical Summary\n",
    "\n",
    "This includes the count, mean, the min and max values as well as some percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptions of each variable\n",
    "\n",
    "# describe() shows some basic statistical details like percentile, mean, std. of a data frame or a series of numeric values.\n",
    "covid_df.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics (minimum, maximum, mean, median, percentiles)\n",
    "\n",
    "print('min:', covid_df['10/11/2020'].min())\n",
    "print('max:', covid_df['10/11/2020'].max())\n",
    "print('mean:', covid_df['10/11/2020'].mean())\n",
    "print('median:', covid_df['10/11/2020'].median())\n",
    "print('50th percentile:', covid_df['10/11/2020'].quantile(0.5)) # 50th percentile, also known as median\n",
    "print('5th percentile:', covid_df['10/11/2020'].quantile(0.05))\n",
    "print('10th percentile:', covid_df['10/11/2020'].quantile(0.1))\n",
    "print('95th percentile:', covid_df['10/11/2020'].quantile(0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using value_counts function to show the number of items\n",
    "\n",
    "print(covid_df['10/11/2020'].value_counts(ascending=True)) # list value counts in ascending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization\n",
    "\n",
    "We now have a basic idea about the data. We need to extend that with some visualizations.\n",
    "\n",
    "We are going to look at two types of plots:\n",
    "\n",
    "1. Histograms\n",
    "2. Bar chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Histograms\n",
    "\n",
    "We can create a histogram to get an idea of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Matplotlib\n",
    "\n",
    "\n",
    "# Plotting a histogram of the 10/11/2020 column values in the Covid-deaths data set\n",
    "plt.hist(covid_df['10/11/2020'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces a bar chart of the 10/11/2020 covid-deaths in the data set.\n",
    "day = covid_df['10/11/2020'].value_counts()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "# Using Numpy\n",
    "\n",
    "\n",
    "# np.arange() returns evenly spaced values within lowest and highest value counts in 10/11/2020\n",
    "# bar() makes the bar plot\n",
    "ax.bar(np.arange(len(day)), day)  \n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Covid Deaths')\n",
    "ax.set_title('Covid Deaths on 10/11/2020')\n",
    "\n",
    "# set_xticks() on the axes will set the data points\n",
    "# set_xticklabels() will set the displayed text\n",
    "ax.set_xticks(np.arange(len(day)))\n",
    "ax.set_xticklabels(day.index)\n",
    "\n",
    "# display Bar chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example was adapted from https://matplotlib.org/gallery/statistics/barchart_demo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Functional API - a way to build graphs of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras functional API is a way to create models that are more flexible. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "\n",
    "The main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras\n",
    "\n",
    "\n",
    "# To build this model using the functional API, start by creating an input node\n",
    "# The shape of the data is set as the shape of our dataset. \n",
    "inputs = keras.Input(covid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs that is returned contains information about the shape and dtype of the input data that you feed to your model\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's the datatype\n",
    "print(inputs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You create a new node in the graph of layers by calling a layer on this inputs object\n",
    "dense = layers.Dense(64, activation=\"relu\")\n",
    "x = dense(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"layer call\" action is like drawing an arrow from \"inputs\" to this layer you created. \n",
    "# You're \"passing\" the inputs to the dense layer, and you get x as the output.\n",
    "\n",
    "# Let's add a few more layers to the graph of layers\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, you can create a Model by specifying its inputs and outputs in the graph of layers\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out what the model summary looks like:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also plot the model as a graph:\n",
    "\n",
    "# NB: To implement this though, you must 'pip intsall pydot'\n",
    "# and install graphviz(https://graphviz.gitlab.io/download/) i.e 'sudo apt install graphviz' for 'pydotprint' to work\n",
    "keras.utils.plot_model(model, \"sample_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"graph of layers\" is an intuitive mental image for a deep learning model, and the functional API is a way to create models that closely mirrors this.\n",
    "\n",
    "This example was adapted from https://keras.io/guides/functional_api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pre-processing data into a form suitable for training\n",
    "\n",
    "This section focuses on the loading, and gives some quick examples of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Tensorflow\n",
    "\n",
    "\n",
    "# Setup\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "\n",
    "# For any small CSV dataset the simplest way to train a TensorFlow model on it is to load it into memory as a pandas Dataframe \n",
    "# or a NumPy array, which we have already done.\n",
    "\n",
    "# Let's assume the nominal task for our dataset is to predict covid deaths from the other measurements, \n",
    "# so we separate the features and labels for training.\n",
    "del covid_df['Country name']\n",
    "covid_features = covid_df.copy()\n",
    "covid_labels = covid_features.pop('09/11/2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this dataset we will treat all features identically. \n",
    "# Pack the features into a single NumPy array.\n",
    "\n",
    "covid_features = np.array(covid_features)\n",
    "covid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next make a regression model predict the covid deaths. \n",
    "# Since there is only a single input tensor, a keras.\n",
    "# Sequential model is sufficient here.\n",
    "\n",
    "covid_model = tf.keras.Sequential([\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "covid_model.compile(loss = tf.losses.MeanSquaredError(),\n",
    "                      optimizer = tf.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train that model, pass the features and labels to Model.fit\n",
    "\n",
    "covid_model.fit(covid_features, covid_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just seen the most basic way to train a model using CSV data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SciPy\n",
    "\n",
    "SciPy is a free and open-source Python library used for scientific computing and technical computing. SciPy contains modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "\n",
    "face = misc.face()\n",
    "plt.imshow(face)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scikit-Learn\n",
    "\n",
    "Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn\n",
    "\n",
    "\n",
    "# We are assuming the variable '09/11/2020' is the target that the model predicts. \n",
    "# All other variables are used as predictors, also called features.\n",
    "\n",
    "# Setup\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "normalize = preprocessing.Normalization()\n",
    "normalize.adapt(covid_features)\n",
    "\n",
    "# Define features as X, target as y.\n",
    "X = covid_labels\n",
    "y = covid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression consists of a coefficient for each feature and one intercept.\n",
    "\n",
    "To make a prediction, each feature is multiplied by its coefficient. The intercept and all of these products are added together. This sum is the predicted value of the target variable.\n",
    "\n",
    "The residual sum of squares (RSS) is calculated to measure the difference between the prediction and the actual value of the target variable.\n",
    "\n",
    "The function fit calculates the coefficients and intercept that minimize the RSS when the regression is used on each record in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6f40cb27e5f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fitting Simple Linear Regression to the Training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# The intercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Fitting Simple Linear Regression to the Training set\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# The intercept\n",
    "print('Intercept: \\n', regressor.intercept_)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', pd.Series(regressor.coef_, index=X.columns, name='coefficients'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
